{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T07:48:29.151848Z",
     "start_time": "2023-11-06T07:48:25.035076Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('popular', quiet=True)\n",
    "import demoji\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform necessary data preprocessing, e.g. removing punctuation and stop words, stemming, lemmatizing. You may use the outputs from previous weekly assignments. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T07:48:33.689737Z",
     "start_time": "2023-11-06T07:48:33.683532Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "def collect_data():\n",
    "    text_file_pattern = \"*.txt\"  # You can adjust the pattern to match your file extensions\n",
    "    text_files = glob.glob(os.path.join(\"../nhs/content\", text_file_pattern))\n",
    "    data = {}\n",
    "    for file_path in text_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_content = file.read()\n",
    "            data[file_name] = file_content\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T07:48:34.436434Z",
     "start_time": "2023-11-06T07:48:34.143558Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = collect_data()\n",
    "text = \"\"\n",
    "for data in corpus:\n",
    "    text += \" \" + data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T07:49:24.301565Z",
     "start_time": "2023-11-06T07:49:24.293752Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Create a translation table to remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Use translate method to remove punctuation\n",
    "    cleaned_text = text.translate(translator)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "    spacy_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "    stop_words = (*nltk_stopwords, *spacy_stopwords, \"NHStxt\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def apply_lemmitization(text):\n",
    "    tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "    tag_map['V'] = wordnet.VERB\n",
    "    tag_map['A'] = wordnet.ADJ\n",
    "    tag_map['R'] = wordnet.ADJ\n",
    "\n",
    "    lemmitizer = WordNetLemmatizer()\n",
    "    lemmitized_result = \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemmitizer.lemmatize(token, tag_map[tag[0]])\n",
    "        lemmitized_result = lemmitized_result + \" \" + lemma\n",
    "    return lemmitized_result\n",
    "\n",
    "def remove_emoji_and_smart_quotes(text):\n",
    "    # replacing emojis with description\n",
    "    text = demoji.replace_with_desc(text)\n",
    "    #Removing smart quotes\n",
    "    return text.replace(\"\"\", \"\\\"\").replace(\"\"\",\"\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T07:49:24.609839Z",
     "start_time": "2023-11-06T07:49:24.598050Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(text):\n",
    "    text = remove_emoji_and_smart_quotes(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = apply_lemmitization(text)\n",
    "    return text\n",
    "\n",
    "def apply_data_preprocessing_to_corpus(corpus):\n",
    "    new_corpus = {}\n",
    "    for idx, key in enumerate(corpus.keys()):\n",
    "        new_corpus[key] = data_preprocessing(corpus[key])\n",
    "        print(f\"idx: {idx}\")\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T07:49:25.050392Z",
     "start_time": "2023-11-06T07:49:25.016286Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_text = data_preprocessing(text)\n",
    "with open('week8_1.txt', 'w') as file:\n",
    "    file.write(f'{processed_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For the binary classification problem you came up last week, set up a MLP to solve it.  (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../nhs/conditions_departments.csv\", header=None)\n",
    "df.columns = [\"index\", \"condition\", \"department\"]\n",
    "department_dict = {\n",
    "    row['condition']: row.drop('condition').to_dict()\n",
    "    for index, row in df.iterrows()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:49:26.184605Z",
     "start_time": "2023-11-06T07:49:25.860653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T07:50:22.700621Z",
     "start_time": "2023-11-06T07:49:26.775253Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get X as text data and Y as 0 if general medicine else 1\n",
    "\"\"\"\n",
    "X = []\n",
    "y =[]\n",
    "for file_name in list(corpus.keys()):\n",
    "    idx = file_name.strip(\" NHS.txt\")\n",
    "    text = data_preprocessing(corpus[file_name])\n",
    "    X.append(text)\n",
    "    y.append(0 if department_dict[idx][\"department\"] == \"General Medicine\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Compute TF-IDF vectors on the text data.  (10 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shireesh/opt/anaconda3/envs/COMP293/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_tokens = 1000\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = .1,\n",
    "                             tokenizer = nltk.word_tokenize,\n",
    "                             max_features = max_tokens)\n",
    "X = vectorizer.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:50:24.598136Z",
     "start_time": "2023-11-06T07:50:22.704747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  data  general\n0      (0, 234)\\t0.0323333686450092\\n  (0, 27)\\t0.0...        0\n1      (0, 153)\\t0.04720455661666712\\n  (0, 306)\\t0...        1\n2      (0, 70)\\t0.06541022666026458\\n  (0, 228)\\t0....        0\n3      (0, 90)\\t0.1164002171842305\\n  (0, 341)\\t0.0...        1\n4      (0, 189)\\t0.03823823168179656\\n  (0, 243)\\t0...        1\n..                                                 ...      ...\n973    (0, 38)\\t0.04855655296617292\\n  (0, 143)\\t0....        0\n974    (0, 296)\\t0.1094379591417735\\n  (0, 161)\\t0....        0\n975    (0, 170)\\t0.046948426395212565\\n  (0, 311)\\t...        0\n976    (0, 38)\\t0.10690698901167914\\n  (0, 143)\\t0....        0\n977    (0, 161)\\t0.029068557808877748\\n  (0, 20)\\t0...        0\n\n[978 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data</th>\n      <th>general</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(0, 234)\\t0.0323333686450092\\n  (0, 27)\\t0.0...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(0, 153)\\t0.04720455661666712\\n  (0, 306)\\t0...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(0, 70)\\t0.06541022666026458\\n  (0, 228)\\t0....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(0, 90)\\t0.1164002171842305\\n  (0, 341)\\t0.0...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(0, 189)\\t0.03823823168179656\\n  (0, 243)\\t0...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>973</th>\n      <td>(0, 38)\\t0.04855655296617292\\n  (0, 143)\\t0....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>974</th>\n      <td>(0, 296)\\t0.1094379591417735\\n  (0, 161)\\t0....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>975</th>\n      <td>(0, 170)\\t0.046948426395212565\\n  (0, 311)\\t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>976</th>\n      <td>(0, 38)\\t0.10690698901167914\\n  (0, 143)\\t0....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>977</th>\n      <td>(0, 161)\\t0.029068557808877748\\n  (0, 20)\\t0...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>978 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"data\":X,\"general\": y})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:50:51.632131Z",
     "start_time": "2023-11-06T07:50:51.553515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:50:56.445402Z",
     "start_time": "2023-11-06T07:50:56.429777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "((782, 376), (196, 376))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:55:04.512837Z",
     "start_time": "2023-11-06T07:55:04.498867Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class BinaryClassifier(pl.LightningModule):\n",
    "    def __init__(self, in_channels):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_channels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        # print(x.shape)\n",
    "        y_hat = self.model(x)\n",
    "        y = y.unsqueeze(1)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if batch_idx == 0:\n",
    "            print(f\"loss {batch_idx}: {loss}\")\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=2e-4)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:33.550780Z",
     "start_time": "2023-11-06T08:29:33.533419Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(torch.tensor(X.toarray(), dtype=torch.float32),\n",
    "                        torch.tensor(y, dtype=torch.float32))\n",
    "# Define the size of your train and test data\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split your dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:34.604752Z",
     "start_time": "2023-11-06T08:29:34.591120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Inference without gradient calculation\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            predicted = outputs.round()  # Using 0.5 as the threshold\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:35.073055Z",
     "start_time": "2023-11-06T08:29:35.047777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "model = BinaryClassifier(X_train.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:36.240547Z",
     "start_time": "2023-11-06T08:29:36.223582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 24.2 K\n",
      "-------------------------------------\n",
      "24.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.2 K    Total params\n",
      "0.097     Total estimated model params size (MB)\n",
      "/Users/shireesh/opt/anaconda3/envs/COMP293/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/shireesh/opt/anaconda3/envs/COMP293/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9f7046b98244fe5970f7f922807b838"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0: 0.7152660489082336\n",
      "loss 0: 0.6991397142410278\n",
      "loss 0: 0.6816490292549133\n",
      "loss 0: 0.6543018221855164\n",
      "loss 0: 0.6283830404281616\n",
      "loss 0: 0.6144261956214905\n",
      "loss 0: 0.5959171652793884\n",
      "loss 0: 0.5644806027412415\n",
      "loss 0: 0.5279189944267273\n",
      "loss 0: 0.5515310764312744\n",
      "loss 0: 0.4709937274456024\n",
      "loss 0: 0.4650184214115143\n",
      "loss 0: 0.46780768036842346\n",
      "loss 0: 0.4079306721687317\n",
      "loss 0: 0.4107171893119812\n",
      "loss 0: 0.4138753414154053\n",
      "loss 0: 0.3341768682003021\n"
     ]
    }
   ],
   "source": [
    "early_stopping = pl.callbacks.EarlyStopping(monitor='train_loss', patience=5, min_delta=1e-6)\n",
    "trainer = pl.Trainer(max_epochs=30, callbacks=[early_stopping])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:40.237314Z",
     "start_time": "2023-11-06T08:29:36.789784Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 85.71%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on the test set\n",
    "accuracy = calculate_accuracy(model, test_loader)\n",
    "print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:44.003394Z",
     "start_time": "2023-11-06T08:29:43.989381Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Try to improve performance by modifying hyperparameters. (30 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class BinaryClassifier2(pl.LightningModule):\n",
    "    def __init__(self, in_channels):\n",
    "        super(BinaryClassifier2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_channels, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        # print(x.shape)\n",
    "        y_hat = self.model(x)\n",
    "        y = y.unsqueeze(1)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if batch_idx == 0:\n",
    "            print(f\"loss {batch_idx}: {loss}\")\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=2e-4)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:33:42.732846Z",
     "start_time": "2023-11-06T08:33:42.721506Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 340 K \n",
      "-------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.363     Total estimated model params size (MB)\n",
      "/Users/shireesh/opt/anaconda3/envs/COMP293/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/shireesh/opt/anaconda3/envs/COMP293/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e05e8fa414e4ab4872c04c8abf52afb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0: 0.6831235289573669\n",
      "loss 0: 0.6509532928466797\n",
      "loss 0: 0.6108098030090332\n",
      "loss 0: 0.5325679779052734\n",
      "loss 0: 0.39180970191955566\n",
      "loss 0: 0.45824435353279114\n",
      "loss 0: 0.30083316564559937\n",
      "loss 0: 0.3549799919128418\n",
      "loss 0: 0.29201117157936096\n",
      "loss 0: 0.2440807819366455\n",
      "loss 0: 0.2251686453819275\n",
      "loss 0: 0.302427738904953\n",
      "loss 0: 0.17919957637786865\n",
      "loss 0: 0.24869504570960999\n",
      "loss 0: 0.4423391819000244\n",
      "loss 0: 0.2803780734539032\n",
      "loss 0: 0.15079404413700104\n",
      "loss 0: 0.21919460594654083\n",
      "loss 0: 0.27004358172416687\n",
      "loss 0: 0.25015029311180115\n",
      "loss 0: 0.2397792935371399\n",
      "loss 0: 0.21643076837062836\n",
      "loss 0: 0.22078466415405273\n",
      "loss 0: 0.2155144214630127\n",
      "loss 0: 0.22596460580825806\n",
      "loss 0: 0.20870733261108398\n",
      "loss 0: 0.17167425155639648\n",
      "loss 0: 0.15788763761520386\n",
      "loss 0: 0.14203757047653198\n",
      "loss 0: 0.18673855066299438\n",
      "loss 0: 0.14141425490379333\n",
      "loss 0: 0.12673766911029816\n",
      "loss 0: 0.22290022671222687\n",
      "loss 0: 0.14004197716712952\n",
      "loss 0: 0.22457602620124817\n",
      "loss 0: 0.09881763160228729\n",
      "loss 0: 0.11778198927640915\n",
      "loss 0: 0.06151909381151199\n",
      "loss 0: 0.0712595134973526\n",
      "loss 0: 0.07483220100402832\n",
      "loss 0: 0.05176405608654022\n",
      "loss 0: 0.050762757658958435\n",
      "loss 0: 0.032004524022340775\n",
      "loss 0: 0.05772785842418671\n",
      "loss 0: 0.02596135064959526\n",
      "loss 0: 0.027589453384280205\n",
      "loss 0: 0.008000734262168407\n",
      "loss 0: 0.03569462522864342\n",
      "loss 0: 0.014005517587065697\n",
      "loss 0: 0.013406251557171345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "model = BinaryClassifier2(X_train.shape[1])\n",
    "trainer = pl.Trainer(max_epochs=50)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:39:29.565221Z",
     "start_time": "2023-11-06T08:39:18.692917Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_accuracy\u001B[49m(model, test_loader)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy of the model on the test set: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'calculate_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(model, test_loader)\n",
    "print(f'Accuracy of the model on the test set: {accuracy:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T00:07:21.483171Z",
     "start_time": "2023-11-15T00:07:20.743758Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Summarize what you have learned and discovered from Task 1-3 as well as the tasks you completed last week.\n",
    "\n",
    "The results from MLP, SVM looks promising compared to SVM and MultinomialNB model\n",
    "\n",
    "| method        | accuracy  |\n",
    "| ------------- | --------- |\n",
    "| mlp           | 90.31%    |\n",
    "| svm           | 90.3%     |\n",
    "| MultinomialNB | 88%       |\n",
    "\n",
    "I would rather choose MLP over SVM cause it can generalise even more data points.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
